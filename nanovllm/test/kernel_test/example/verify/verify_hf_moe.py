import torch
import torch.nn as nn
import torch.nn.functional as F
import triton
import triton.language as tl
import functools
from typing import Optional, Tuple

#
# ============================================================================
#
#               ç¬¬ä¸€éƒ¨åˆ†: PyTorch å‚è€ƒæ¨¡å‹ (æ‚¨æä¾›çš„ä»£ç )
#
# ============================================================================
#
# ACT2FN æ˜¯ä¸€ä¸ªæ¿€æ´»å‡½æ•°çš„æ˜ å°„ï¼Œæˆ‘ä»¬è¿™é‡Œä¸ºäº†ç®€å•ç›´æ¥å®šä¹‰
# åœ¨å®é™… Transformers åº“ä¸­ï¼Œè¿™æ˜¯ä¸€ä¸ªå­—å…¸
class SiLU(nn.Module):
    def forward(self, x):
        return F.silu(x)

ACT2FN = {"silu": SiLU()}


class MixtralBlockSparseTop2MLP(nn.Module):
    def __init__(self, hidden_dim, ffn_dim, hidden_act):
        super().__init__()
        self.ffn_dim = ffn_dim
        self.hidden_dim = hidden_dim

        self.w1 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)
        self.w2 = nn.Linear(self.ffn_dim, self.hidden_dim, bias=False)
        self.w3 = nn.Linear(self.hidden_dim, self.ffn_dim, bias=False)

        self.act_fn = ACT2FN[hidden_act]

    def forward(self, hidden_states):
        current_hidden_states = self.act_fn(self.w1(hidden_states)) * self.w3(hidden_states)
        current_hidden_states = self.w2(current_hidden_states)
        return current_hidden_states


class MixtralSparseMoeBlock(nn.Module):
    """çº¯ PyTorch å®ç°çš„ MoE æ¨¡å—ï¼Œä½œä¸ºâ€œé»„é‡‘æ ‡å‡†â€"""
    def __init__(self, config):
        super().__init__()
        self.hidden_dim = config['hidden_size']
        self.ffn_dim = config['intermediate_size']
        self.num_experts = config['num_experts']
        self.top_k = config['top_k']

        self.gate = nn.Linear(self.hidden_dim, self.num_experts, bias=False)
        self.experts = nn.ModuleList([
            MixtralBlockSparseTop2MLP(self.hidden_dim, self.ffn_dim, config['hidden_act']) 
            for _ in range(self.num_experts)
        ])
        self.jitter_noise = 0 # æ¨ç†å’ŒéªŒè¯æ—¶ç¦ç”¨

    def forward(self, hidden_states: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        batch_size, sequence_length, hidden_dim = hidden_states.shape
        hidden_states_reshaped = hidden_states.view(-1, hidden_dim)
        print(f"M : hidden_state[0:10]:{hidden_states_reshaped[0,:10]}; hidden_states_reshaped {hidden_states_reshaped.shape}")
        router_logits = self.gate(hidden_states_reshaped)
        print(f"M : router_logits[0:]:{router_logits[0,:]}")
        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)
        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)
        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)
        routing_weights = routing_weights.to(hidden_states.dtype)
        print(f"M : routing_weights[0:]:{routing_weights[0,:]} ; selected_experts[0:] :{selected_experts[0:]}")

        final_hidden_states = torch.zeros_like(hidden_states_reshaped)
        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)

        for expert_idx in range(self.num_experts):
            expert_layer = self.experts[expert_idx]
            idx, top_x = torch.where(expert_mask[expert_idx])

            if top_x.shape[0] == 0:
                continue

            current_state = hidden_states_reshaped[top_x]
            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx].unsqueeze(1)
            final_hidden_states.index_add_(0, top_x, current_hidden_states)
            
        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)
        return final_hidden_states, router_logits

#
# ============================================================================
#
#               ç¬¬äºŒéƒ¨åˆ†: æ‚¨çš„ Fused MoE å®ç° (æ‚¨æä¾›çš„ä»£ç )
#
# ============================================================================
#
import electrock_infer
from nanovllm.layers.moe.moe_align_block_size import moe_align_block_size
# from nanovllm.test.kernel_test.example.verify.fused_moe import fused_experts
from nanovllm.layers.moe.fused_moe import fused_experts
# @triton.jit def fused_moe_kernel(...): ...
# def fused_topk(...): ...
# def fused_experts(...): ...
# def _invoke_fused_moe_kernel(...): ...
# def _get_moe_config(...): ...
# (ä»¥ä¸Šå‡½æ•°è¯·ç¡®ä¿å·²åœ¨æ­¤æ–‡ä»¶æˆ–å¯¼å…¥çš„æ–‡ä»¶ä¸­å®šä¹‰)


#
# ============================================================================
#
#               ç¬¬ä¸‰éƒ¨åˆ†: ä¸»éªŒè¯é€»è¾‘
#
# ============================================================================
#

def run_verification():
    """æ‰§è¡Œç«¯åˆ°ç«¯çš„æ­£ç¡®æ€§éªŒè¯"""
    
    # --- 1. é…ç½®æµ‹è¯•å‚æ•° ---
    BATCH_SIZE = 1
    NUM_TOKENS = 1 # éªŒè¯æ—¶é€šå¸¸ç”¨ 1
    HIDDEN_SIZE = 4096
    INTERMEDIATE_SIZE = 14336 // 2 # ä¿®æ­£ï¼šåº”è¯¥æ˜¯ 14336/2
    NUM_EXPERTS = 8
    TOP_K = 2
    DTYPE = torch.bfloat16
    DEVICE = 'cuda'

    config = {
        'hidden_size': HIDDEN_SIZE,
        'intermediate_size': INTERMEDIATE_SIZE,
        'num_experts': NUM_EXPERTS,
        'top_k': TOP_K,
        'hidden_act': 'silu'
    }
    
    print("="*80)
    print("Starting End-to-End MoE Correctness Verification")
    print(f"Config:  {BATCH_SIZE=} {NUM_TOKENS=}, {HIDDEN_SIZE=}, {INTERMEDIATE_SIZE=}, {NUM_EXPERTS=}, {TOP_K=}, DType={DTYPE}")
    print("="*80)

    # --- 2. åˆ›å»ºå‚è€ƒæ¨¡å‹å¹¶è®¾ç½®æƒé‡ ---
    torch.manual_seed(42) # å›ºå®šéšæœºç§å­ä»¥ä¿è¯æ¯æ¬¡æƒé‡ä¸€è‡´
    reference_model = MixtralSparseMoeBlock(config).to(DEVICE, dtype=DTYPE).eval()

    # --- 3. å‡†å¤‡å®Œå…¨ç›¸åŒçš„è¾“å…¥ ---
    hidden_states = torch.randn((BATCH_SIZE, NUM_TOKENS, HIDDEN_SIZE), dtype=DTYPE, device=DEVICE)

    # --- 4. è¿è¡Œå‚è€ƒæ¨¡å‹ï¼Œå¾—åˆ°â€œé»„é‡‘æ ‡å‡†â€ç»“æœ ---
    print("Running PyTorch reference implementation...")
    with torch.no_grad():
        expected_output, expected_logits = reference_model(hidden_states)
    print("Reference implementation finished.")
    expected_output = expected_output.reshape(-1, HIDDEN_SIZE)
    print(f"Reference reuslt shapre : {expected_output.shape}")
    # --- 5. å‡†å¤‡ Fused MoE çš„è¾“å…¥ï¼Œå…³é”®æ˜¯ç»Ÿä¸€æƒé‡ ---
    print("\nPreparing inputs for Fused MoE implementation...")
    # a. é—¨æ§ç½‘ç»œçš„è¾“å‡ºå°±æ˜¯å‚è€ƒæ¨¡å‹çš„ logits
    gating_output = expected_logits.view(-1, NUM_EXPERTS)

    # b. å †å ä¸“å®¶æƒé‡
    # w1 å’Œ w3 åˆå¹¶ä¸º w1_stacked
    w1_stacked_list = [torch.cat([expert.w1.weight, expert.w3.weight], dim=0) for expert in reference_model.experts]
    w1_stacked = torch.stack(w1_stacked_list)
    print(f"w1_stacked shape {w1_stacked.shape}\n")
    # w2 å †å ä¸º w2_stacked
    w2_stacked_list = [expert.w2.weight for expert in reference_model.experts]
    w2_stacked = torch.stack(w2_stacked_list)
    print(f"w2_stacked shape {w2_stacked.shape}\n")
    print("Stacked weights prepared.")
    
    # --- 6. è¿è¡Œæ‚¨çš„ Fused MoE å®ç° ---
    print("\nRunning your Fused MoE implementation...")
    with torch.no_grad():
        actual_output = fused_experts(
            hidden_states=hidden_states.view(-1, HIDDEN_SIZE),
            w1=w1_stacked,
            w2=w2_stacked,
            gating_output=gating_output,
            topk=TOP_K,
            renormalize=True, # å¿…é¡»ä¸å‚è€ƒæ¨¡å‹é€»è¾‘ä¸€è‡´
            inplace=True,
        )
    print("Your implementation finished.")
    print(f"Your finished reuslt shapre : {actual_output.shape}")
    # --- 7. å¯¹æ¯”æœ€ç»ˆç»“æœ ---
    print("\nComparing final outputs...")
    # å°†è¾“å‡º reshape æˆç›¸åŒå½¢çŠ¶ä»¥ä¾¿æ¯”è¾ƒ
    actual_output_reshaped = actual_output.view(-1, HIDDEN_SIZE)
    
    # ä½¿ç”¨ torch.allclose è¿›è¡Œæµ®ç‚¹æ•°æ¯”è¾ƒ
    is_correct = torch.allclose(expected_output, actual_output_reshaped, atol=1e-1, rtol=1e-2)

    if is_correct:
        print("\nâœ…âœ…âœ… Verification PASSED! âœ…âœ…âœ…")
        print("Your `fused_experts` implementation is numerically consistent with the reference PyTorch model.")
        print("\nExpected output (first token, first 5 values):")
        print(expected_output[0, :20])
        print("\nActual output (first token, first 5 values):")
        print(actual_output_reshaped[0, :20])
    else:
        print("\nâŒâŒâŒ Verification FAILED! âŒâŒâŒ")
        max_diff = torch.max(torch.abs(expected_output - actual_output_reshaped))
        print(f"Maximum absolute difference between outputs: {max_diff.item()}")
        
        # æ‰“å°ä¸€äº›å€¼ç”¨äºè°ƒè¯•
        print("\nExpected output (first token, first 5 values):")
        print(expected_output[0, :20])
        print("\nActual output (first token, first 5 values):")
        print(actual_output_reshaped[0, :20])

    print("="*80)

def run_performance_analysis(num_tokens=1024, num_repeats=1, warmup_steps=0):
    """æ‰§è¡Œç®€åŒ–çš„æ€§èƒ½åˆ†æï¼Œåªå…³æ³¨è€—æ—¶ã€‚"""
    
    # --- 1. é…ç½®ä¸ç¯å¢ƒå‡†å¤‡ ---
    BATCH_SIZE = 64
    HIDDEN_SIZE = 4096
    INTERMEDIATE_SIZE = 14336 // 2
    NUM_EXPERTS = 8
    TOP_K = 2
    DTYPE = torch.bfloat16
    DEVICE = 'cuda'

    config = {
        'hidden_size': HIDDEN_SIZE,
        'intermediate_size': INTERMEDIATE_SIZE,
        'num_experts': NUM_EXPERTS,
        'top_k': TOP_K,
        'hidden_act': 'silu'
    }

    print("\n" + "="*80)
    print("å¼€å§‹æ€§èƒ½åˆ†æ (ä»…è®¡æ—¶)")
    print(f"é…ç½®: num_tokens={num_tokens}, é‡å¤æ¬¡æ•°={num_repeats}, é¢„çƒ­æ¬¡æ•°={warmup_steps}")
    print("="*80)

    # --- 2. å‡†å¤‡æ¨¡å‹ä¸è¾“å…¥æ•°æ® ---
    torch.manual_seed(1000)
    reference_model = MixtralSparseMoeBlock(config).to(DEVICE, dtype=DTYPE).eval()
    hidden_states = torch.randn((BATCH_SIZE, num_tokens, HIDDEN_SIZE), dtype=DTYPE, device=DEVICE)
    
    with torch.no_grad():
        _, gating_logits = reference_model(hidden_states)
    gating_output = gating_logits.view(-1, NUM_EXPERTS)
    
    w1_stacked = torch.stack([torch.cat([expert.w1.weight, expert.w3.weight], dim=0) for expert in reference_model.experts])
    w2_stacked = torch.stack([expert.w2.weight for expert in reference_model.experts])
    hidden_states_flat = hidden_states.view(-1, HIDDEN_SIZE).clone()
    
    # --- 3. åˆå§‹åŒ–CUDAäº‹ä»¶ç”¨äºç²¾ç¡®è®¡æ—¶ ---
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)

    # --- 4. æµ‹è¯• PyTorch å‚è€ƒæ¨¡å‹ ---
    print("\n--- æµ‹è¯• PyTorch å‚è€ƒæ¨¡å‹ ---")
    with torch.no_grad():
        for _ in range(warmup_steps): # é¢„çƒ­
            _ = reference_model(hidden_states)


        start_event.record() # å¼€å§‹è®¡æ—¶
        for _ in range(num_repeats):
            _ = reference_model(hidden_states)
        end_event.record() # ç»“æŸè®¡æ—¶

    ref_avg_latency = start_event.elapsed_time(end_event) / num_repeats
    print(f"å¹³å‡è€—æ—¶: {ref_avg_latency:.4f} ms")

    # --- 5. æµ‹è¯• Fused MoE å®ç° ---
    print("\n--- æµ‹è¯• Fused MoE å®ç° ---")
    with torch.no_grad():
        for _ in range(warmup_steps): # é¢„çƒ­
            _ = fused_experts(hidden_states_flat.clone(), w1_stacked, w2_stacked, gating_output, TOP_K, inplace=False)

        torch.cuda.synchronize()
        start_event.record() # å¼€å§‹è®¡æ—¶
        for _ in range(num_repeats):
            _ = fused_experts(hidden_states_flat.clone(), w1_stacked, w2_stacked, gating_output, TOP_K, inplace=False)
        end_event.record() # ç»“æŸè®¡æ—¶
        torch.cuda.synchronize()

    fused_avg_latency = start_event.elapsed_time(end_event) / num_repeats
    print(f"å¹³å‡è€—æ—¶: {fused_avg_latency:.4f} ms")

    # --- 6. æ€»ç»“ ---
    print("\n" + "-"*40)
    print("æ€§èƒ½æ€»ç»“")
    print("-"*40)
    speedup = ref_avg_latency / fused_avg_latency
    print(f"ğŸš€ åŠ é€Ÿæ¯”: Fused MoE å®ç°é€Ÿåº¦æ˜¯ PyTorch å‚è€ƒçš„ {speedup:.2f} å€ã€‚")
    print("="*80)

if __name__ == "__main__":
    # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„å‡½æ•°å’Œç±»éƒ½å·²å®šä¹‰æˆ–å¯¼å…¥
    run_verification()
    # run_performance_analysis()