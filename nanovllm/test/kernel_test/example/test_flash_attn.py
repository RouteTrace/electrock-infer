import torch
import torch.nn.functional as F
import time
import sys

# ==============================================================================
# é‡è¦ï¼šè¯·å°† 'electrock_infer' æ›¿æ¢ä¸ºæ‚¨ç¼–è¯‘çš„C++æ‰©å±•çš„å®žé™…åç§°
# ==============================================================================
try:
    # å‡è®¾æ‚¨çš„ç»‘å®šæ¨¡å—åä¸º electrock_infer
    import electrock_infer as custom_ops
except ImportError:
    print("âŒ æ— æ³•å¯¼å…¥è‡ªå®šä¹‰C++æ‰©å±•ã€‚")
    print("   è¯·ç¡®ä¿å·²ç»æˆåŠŸç¼–è¯‘ï¼Œå¹¶ä¸”è¿™é‡Œçš„æ¨¡å—åç§°ä¸Žæ‚¨setup.pyä¸­å®šä¹‰çš„ä¸€è‡´ã€‚")
    sys.exit(1)

def repeat_kv(x: torch.Tensor, group_size: int) -> torch.Tensor:
    """
    è¾…åŠ©å‡½æ•°ï¼Œç”¨äºŽGQAä¸­é‡å¤Kå’ŒVå¼ é‡ï¼Œä½¿å…¶ä¸ŽMHAå…¼å®¹ã€‚
    """
    if group_size == 1:
        return x
    
    batch_size, num_kv_heads, seq_len, head_dim = x.shape
    
    return (
        x.unsqueeze(2)  # [B, h_kv, 1, S, D]
         .expand(batch_size, num_kv_heads, group_size, seq_len, head_dim)  # [B, h_kv, G, S, D]
         .reshape(batch_size, num_kv_heads * group_size, seq_len, head_dim)  # [B, h_kv*G, S, D]
    )

def verify_gqa_python_preallocated_output():
    # --- 1. GQA å‚æ•°è®¾ç½® ---
    NUM_Q_HEADS = 32
    NUM_KV_HEADS = 8
    HEAD_DIM = 128
    GROUP_SIZE = NUM_Q_HEADS // NUM_KV_HEADS
    
    BATCH_SIZE = 64
    SEQ_LEN = 128
    IS_CAUSAL = True

    # æ€§èƒ½è¯„æµ‹å‚æ•°
    WARMUP_ITER = 5
    TEST_ITER = 20

    if not torch.cuda.is_available():
        print("âŒ æœªæ‰¾åˆ°CUDAè®¾å¤‡ï¼Œç¨‹åºä¸­æ­¢ã€‚")
        return

    device = "cuda"
    dtype = torch.half

    print("===========================================================")
    print("ðŸš€ å¼€å§‹ (Python) GQA FlashAttention éªŒè¯ä¸ŽåŸºå‡†æµ‹è¯•")
    print(f"é…ç½®: B={BATCH_SIZE}, H_Q={NUM_Q_HEADS}, H_KV={NUM_KV_HEADS}, N={SEQ_LEN}, d={HEAD_DIM}")
    print(f"åˆ†ç»„å¤§å° (Group Size): {GROUP_SIZE}")
    print("===========================================================")

    # --- 2. åˆ›å»ºGQAè¾“å…¥æ•°æ® ---
    # Qçš„å½¢çŠ¶ä½¿ç”¨ NUM_Q_HEADS
    q = torch.randn(BATCH_SIZE, NUM_Q_HEADS, SEQ_LEN, HEAD_DIM, device=device, dtype=dtype)
    # Kå’ŒVçš„å½¢çŠ¶ä½¿ç”¨ NUM_KV_HEADS
    k = torch.randn(BATCH_SIZE, NUM_KV_HEADS, SEQ_LEN, HEAD_DIM, device=device, dtype=dtype)
    v = torch.randn(BATCH_SIZE, NUM_KV_HEADS, SEQ_LEN, HEAD_DIM, device=device, dtype=dtype)
    
    # --- å…³é”®æ”¹åŠ¨ï¼šé¢„å…ˆåˆ†é…å¥½ç”¨äºŽæŽ¥æ”¶ç»“æžœçš„è¾“å‡ºå¼ é‡ O ---
    o_custom = torch.empty_like(q)

    # --- 3. æ­£ç¡®æ€§éªŒè¯ ---
    print("\n[1/4] æ­£åœ¨æ‰§è¡Œæ­£ç¡®æ€§æ£€æŸ¥...")
    
    # PyTorch å®˜æ–¹å‚è€ƒå®žçŽ°
    k_repeat = repeat_kv(k, GROUP_SIZE)
    v_repeat = repeat_kv(v, GROUP_SIZE)
    o_torch_ref = F.scaled_dot_product_attention(
        q, k_repeat, v_repeat, is_causal=IS_CAUSAL
    )

    # æ‚¨çš„è‡ªå®šä¹‰æ ¸å‡½æ•°è®¡ç®—
    # æ³¨æ„ï¼šçŽ°åœ¨ o_custom æ˜¯ä½œä¸ºæœ€åŽä¸€ä¸ªå‚æ•°ä¼ å…¥
    if IS_CAUSAL:
        custom_ops.flash_attn_simplified_causal_varlen_gqa(q, k, v, o_custom)
    else:
        # å‡è®¾æ‚¨çš„éžå› æžœGQAæ ¸å‡½æ•°ä¹Ÿå·²ç»‘å®šã€‚å¦‚æžœæ²¡æœ‰ï¼Œè¿™é‡Œéœ€è¦æ›¿æ¢æˆå¯¹åº”çš„è°ƒç”¨
        # è¿™é‡Œæˆ‘ä»¬ç»§ç»­ä½¿ç”¨ causal å˜ä½“ä½œä¸ºæ¼”ç¤º
        custom_ops.flash_attn_simplified_causal_varlen_gqa(q, k, v, o_custom)

    torch.cuda.synchronize()

    try:
        torch.testing.assert_close(o_custom, o_torch_ref, rtol=1e-3, atol=1e-2)
        print("âœ… æ­£ç¡®æ€§æ£€æŸ¥é€šè¿‡ï¼")
        diff = (o_custom - o_torch_ref).abs()
        print(f"   - æœ€å¤§ç»å¯¹è¯¯å·®: {diff.max().item():.6f}")
    except AssertionError as e:
        print("âŒ æ­£ç¡®æ€§æ£€æŸ¥å¤±è´¥ï¼è¾“å‡ºä¸åŒ¹é…ã€‚")
        diff = (o_custom - o_torch_ref).abs()
        print(f"   - æœ€å¤§ç»å¯¹è¯¯å·®: {diff.max().item():.6f}")
        return

    # --- 4. æ€§èƒ½è¯„æµ‹ ---
    print("\n[2/4] æ­£åœ¨é¢„çƒ­ (Warmup) å†…æ ¸...")
    for _ in range(WARMUP_ITER):
        # é¢„çƒ­æ‚¨çš„æ ¸å‡½æ•°
        custom_ops.flash_attn_simplified_causal_varlen_gqa(q, k, v, o_custom)
        # é¢„çƒ­PyTorchå‚è€ƒå®žçŽ°
        k_rep, v_rep = repeat_kv(k, GROUP_SIZE), repeat_kv(v, GROUP_SIZE)
        _ = F.scaled_dot_product_attention(q, k_rep, v_rep, is_causal=IS_CAUSAL)
    torch.cuda.synchronize()
    print("âœ… é¢„çƒ­å®Œæˆã€‚")

    # a) è¯„æµ‹æ‚¨çš„è‡ªå®šä¹‰GQAå†…æ ¸
    print("\n[3/4] æ­£åœ¨è¯„æµ‹æ‚¨çš„è‡ªå®šä¹‰GQAå†…æ ¸...")
    start_event = torch.cuda.Event(enable_timing=True)
    end_event = torch.cuda.Event(enable_timing=True)
    
    start_event.record()
    for _ in range(TEST_ITER):
        custom_ops.flash_attn_simplified_causal_varlen_gqa(q, k, v, o_custom)
    end_event.record()
    
    torch.cuda.synchronize()
    custom_kernel_ms = start_event.elapsed_time(end_event)
    avg_custom_ms = custom_kernel_ms / TEST_ITER
    print("âœ… è‡ªå®šä¹‰GQAå†…æ ¸è¯„æµ‹å®Œæˆã€‚")

    # b) è¯„æµ‹PyTorchå®˜æ–¹GQAå®žçŽ°
    print("\n[4/4] æ­£åœ¨è¯„æµ‹PyTorchå®˜æ–¹GQAå®žçŽ°...")
    start_event.record()
    for _ in range(TEST_ITER):
        k_rep = repeat_kv(k, GROUP_SIZE)
        v_rep = repeat_kv(v, GROUP_SIZE)
        _ = F.scaled_dot_product_attention(q, k_rep, v_rep, is_causal=IS_CAUSAL)
    end_event.record()

    torch.cuda.synchronize()
    ref_kernel_ms = start_event.elapsed_time(end_event)
    avg_ref_ms = ref_kernel_ms / TEST_ITER
    print("âœ… PyTorchå®˜æ–¹GQAå®žçŽ°è¯„æµ‹å®Œæˆã€‚")

    # --- 5. æ‰“å°æ€§èƒ½å¯¹æ¯”ç»“æžœ ---
    print("\n===========================================================")
    print("ðŸ“Š GQA åŸºå‡†æµ‹è¯•ç»“æžœ")
    print("===========================================================")
    print(f"æ‚¨çš„è‡ªå®šä¹‰GQAå†…æ ¸ : {avg_custom_ms:.4f} ms / è¿­ä»£")
    print(f"PyTorchå®˜æ–¹å‚è€ƒ   : {avg_ref_ms:.4f} ms / è¿­ä»£")
    print("-----------------------------------------------------------")
    if avg_custom_ms > 0:
        speedup = avg_ref_ms / avg_custom_ms
        print(f"ðŸš€ åŠ é€Ÿæ¯” vs PyTorch : {speedup:.2f}x")
    print("===========================================================")


if __name__ == "__main__":
    verify_gqa_python_preallocated_output()